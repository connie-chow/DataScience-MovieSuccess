# -*- coding: utf-8 -*-
"""MovieSuccess-ExploratoryDataAnalysisModeling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v50QKC30OmLJDUkOUrTD-w1AbHMurNFY

Name: Sau Chow 100% - Student ID: 000734527
Name: Weifeng Ma 0%

Project Name: Movie Success Prediction Project
Class: CMPE255

This section includes exploratory data analysis and prediction models + results
"""

!pip install dython
!pip install cpi

import pandas as pd
import numpy as np
import cpi
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import MinMaxScaler
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, plot_confusion_matrix
from sklearn.metrics import plot_roc_curve
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import MultinomialNB
from dython import nominal

from google.colab import files
uploaded = files.upload()

#Imports and Load Dataset 
#Load dataset after merging fields OMdb_merged.csv

pd.set_option('max_columns', None)

OMdb = pd.read_csv('OMdb_merged.csv')

OMdb.head(2)

"""### Data Preprocessing"""

'''Adjust the worldwide-gross and budget column values for inflation
https://medium.com/analytics-vidhya/adjusting-for-inflation-when-analysing-historical-dat a-with-python-9d69a8dcbc27
'''

cpi.update()
def inflate_column(data, column): 
    return data.apply(lambda x:cpi.inflate(x[column], x.Year), axis = 1)

OMdb['real_budget'] = inflate_column(OMdb, 'budget')
OMdb['real_revenue'] = inflate_column(OMdb, 'worldwide-gross')

"""ref: https://medium.com/analytics-vidhya/adjusting-for-inflation-when-analysing-historical-data-with-python-9d69a8dcbc27

Drop redundant columns:
1. Actors and Director
2. titles, mojo_title, and search_strs
3.  IMdb_ID, TMdb_id and itunes_id
"""

OMdb.columns

# Title and titles are the same, but titles also include year. We already have a column Year so I'm deleting titles. 
OMdb.loc[OMdb['Title']!= OMdb['titles'], ['Title', 'titles']]

# Same thing, deleting mojo_title
OMdb.loc[OMdb['Title']!= OMdb['mojo_title'], ['Title', 'mojo_title']]

# same thing as search_strs. 
OMdb['search_strs']

new_OMdb = OMdb.drop(columns = ['Actors', 'Director', 'titles', 'mojo_title', 'search_strs', 'IMdb_ID','itunes_id', 'TMdb_id'])

new_OMdb.columns

# change column names for groupby to work in the feature selection. 
OMdb = new_OMdb.rename(columns = {
    'bo_year':'bo-year',
    'bo_year_rank': 'bo-year-rank',
    'num_actor': 'num-actor',
    'num_director': 'num-director',
    'num_genre': 'num-genre',
    'oscar_noms': 'oscar-noms',
    'oscar_wins': 'oscar-wins',
    'IMdb_score': 'IMdb-score',
    'num_lang': 'num-lang',
    'RT_score': 'RT-score',
    'real_budget': 'real-budget'
})

"""Exploratory Data Analysis"""

corr_df = OMdb[['Awards', 'Runtime','IMdb-score', 'worldwide-gross', 'director_2', 'Production', 'studio',
               'imdbVotes','Rated', 'oscar-noms', 'nominations','Writer 4', 'oscar-wins', 'Writer 1',
               'Writer 3', 'Writer 2', 'Language']]

nominal.associations(corr_df, nominal_columns = 'all', figsize=(15, 15), annot =True)

#for only numeric data
plt.figure(figsize=(8, 13))
cov_omdb = OMdb.cov().loc[:,['worldwide-gross']].sort_values(by='worldwide-gross', ascending = False)
sns.heatmap(cov_omdb, annot = True, vmin = -1, vmax =1)
plt.savefig('OMdbcov for numeric data.png')

"""Since movie runtime is in units of minutes (100 minutes) and revenue/budget is in millions of dollars ($100M), the dataset columns will need to be normalized."""

scaler = MinMaxScaler()
OMdb_scaled = scaler.fit_transform(OMdb._get_numeric_data())

norm = pd.DataFrame(OMdb_scaled, columns = OMdb._get_numeric_data().columns)

norm.head(2)

def replace_norms(OMdb_df, norm_df):
    cols = norm_df.columns
    for col in cols: 
        OMdb_df[col] = norm_df[col]

replace_norms(OMdb, norm)

OMdb.head(2)

"""Run onehotencoding on the normalized and adjusted dataset"""

cat_cols = np.array(pd.DataFrame(OMdb.dtypes[OMdb.dtypes == 'object']).index)

ohe = OneHotEncoder(drop = 'first')
ohe_array = ohe.fit_transform(OMdb[cat_cols]).toarray()
ohe_OMdb = pd.DataFrame(ohe_array, index = OMdb.index, columns = ohe.get_feature_names(cat_cols ))

ohe_OMdb.to_csv('ohe_cols.csv', index = False)

from google.colab import files
files.download("ohe_cols.csv")

OMdb_drop_col = OMdb.drop(columns = cat_cols)

OMdb_ohed = pd.concat([OMdb_drop_col, ohe_OMdb], axis = 1)

OMdb_ohed.columns

'''
Plot revenue by genre. Revenue comes from “worldwide-gross” and “Genre” in OMDb_mojo_clean.csv. 
'''

OMdb['genre_1'] = [i.split(',')[0] for i in OMdb['Genre'] ]
genre_revenue_df= pd.DataFrame(OMdb.groupby(['genre_1']).sum()['real_revenue'])
plt.figure(figsize = (10, 5))
plt.bar(genre_revenue_df.index, genre_revenue_df['real_revenue'])
plt.title('Revenue By Genre')
plt.xlabel('Genre')
plt.ylabel('Normalized Real Revenue')
plt.savefig('real_revenue_by_genre.png');

'''
Take all the movies and bin them by the month that they are released. 
Then plot them against the revenue generated per movie.
'''
month = []
for data in OMdb['Released']:
    if data != '0':
        month.append(pd.to_datetime(data).month)
    else:
        month.append(0)
        
OMdb['month'] = month
month_revenue_df =pd.DataFrame(OMdb.groupby(by = ['month', 'Title']).sum()['real_revenue'])
month_revenue_df.reset_index(inplace = True)
plt.scatter(month_revenue_df['month'], month_revenue_df['real_revenue'])
plt.title('Real Revernue \n Generated per Movie per Month')
plt.xlabel('Month')
plt.grid(True, which='major', axis='y')
plt.ylabel('Normalized Real Revenue')
plt.savefig('real_revenue_per_movie_per_month.png');

'''
(budget/revenue) * 100% equals percentage return on a movie
bin the movies by the director
'''
OMdb['pct_return'] = OMdb['real-budget']/OMdb['real_revenue']*100
revenue_director_df = pd.DataFrame(OMdb.groupby(by=['director_1']).mean()['pct_return'])
plt.figure(figsize = (35,7))
plt.bar(revenue_director_df.index, revenue_director_df['pct_return'])
plt.title('Real Percentage Return on Average per Director', fontsize = 30)
plt.xlabel('Director', fontsize = 15)
plt.ylabel('Real Percentage Return', fontsize = 15)
plt.xticks(rotation = 90)
plt.savefig('real_percentage_return_per_director.png');

# Worldwide-gross to production studio
wwg_production_df = pd.DataFrame(OMdb.groupby(by=['Production']).sum()['real_revenue'])
wwg_studio_df = pd.DataFrame(OMdb.groupby(by=['studio']).sum()['real_revenue'])

plt.figure(figsize = (30,15))
ax1 = plt.subplot(2,1,1)
ax1.set_title('Real Revenue by Production', fontsize = 30)
ax1.bar(wwg_production_df.index, wwg_production_df['real_revenue'])
ax1.set_xlabel('Production', fontsize = 15)
ax1.set_xticklabels(wwg_production_df.index, rotation = 90)
ax1.set_ylabel('Normalized Real Revenue', fontsize = 15)

ax2 = plt.subplot(2,1,2)
ax2.set_title('Real Revenue by Studio', fontsize = 30)
ax2.bar(wwg_studio_df.index, wwg_studio_df['real_revenue'])
ax2.set_xlabel('Studio', fontsize = 15)
ax2.set_xticklabels(wwg_studio_df.index, rotation = 90)
ax2.set_ylabel('Normalized Real Revenue', fontsize = 15)

plt.tight_layout()
plt.savefig('real revenue to production vs. studio.png')

#Worldwide-gross to total # awards won
plt.scatter(OMdb['awards'], OMdb['real_revenue'])
plt.title('Real Revenue to Number of Oscar Wins')
plt.xlabel('Number of Awards')
plt.ylabel('Normalized Real Revenue')
plt.savefig('Real Revenue to Number of Oscar Wins.png');

# Real Revenue to actor_1
wwg_actor1_df = pd.DataFrame(OMdb.groupby(by=['actor_1']).sum()['real_revenue'])
plt.figure(figsize = (35,7))
plt.bar(wwg_actor1_df.index, wwg_actor1_df['real_revenue'])
plt.title('Real Revenue by actor1', fontsize = 30)
plt.xlabel('Actor Name', fontsize = 15)
plt.ylabel('Normalized Real Revenue', fontsize = 15)
plt.xticks(rotation = 90)
plt.savefig('Real Revenue to actor1.png');

# Real Revenue to writer_1
wwg_writer_df = pd.DataFrame(OMdb.groupby(by=['Writer 1']).sum()['real_revenue'])
plt.figure(figsize = (40,7))
plt.bar(wwg_writer_df.index, wwg_writer_df['real_revenue'])
plt.title('Real Revenue by Writer_1', fontsize = 30)
plt.xlabel('Writer Name', fontsize = 15)
plt.ylabel('Normalized Real Revenue', fontsize = 15)
plt.xticks(rotation = 90)
plt.savefig('Real Revenue to writer_1.png');

"""Prediction Models

Criteria for BinaryClassification:
Target/label = 1 if worldwide-gross >= median of the worldwide-gross column (adjusted for inflation)
0 if otherwise
"""

revenue_median = OMdb_ohed['real_revenue'].median()
OMdb_ohed['target_revenue']= OMdb_ohed['real_revenue'].map(lambda x: 1 if x >= revenue_median else 0)

revenue_median

"""Feature Selection / Dimensionality Reduction:
Use SelectKBest to determine the highest impact features or combination of
"""

# For the feature columns I dropped worldwide-gross because real_revenue comes from worldwide-gross
# I also droped domestic-gross and overseas-gross because worldwide-gross = domestic-gross + overseas-gross
X = OMdb_ohed.drop(columns = ['worldwide-gross','target_revenue','domestic-gross','overseas-gross','real_revenue'])
y = OMdb_ohed['target_revenue']

# Select the top 20 features using SelectKbest
skb = SelectKBest(chi2, k = 50)
skb = skb.fit(X, y)
df_scores = pd.DataFrame(skb.scores_)
df_columns = pd.DataFrame(X.columns)
feature_df = pd.concat([df_columns, df_scores],axis=1)
feature_df.columns = ['features', 'score']

feature_df['group_col'] = [each.split('_')[0] for each in feature_df['features']]

new_feature_df = feature_df.groupby(by= 'group_col').mean().sort_values(by = 'score', ascending =False)

top20_df = feature_df.nlargest(20, 'score')
top20_df = top20_df.sort_values(by = 'score', ascending = True )

plt.figure(figsize=(7,7))
plt.barh(top20_df['features'], top20_df['score'])
plt.title('Top 20 Features with the Highest Impact', fontsize = 15)
plt.savefig('selected features and impact socre.png');

top_group = pd.DataFrame(top20_df.groupby(by = 'group_col').mean()['score']).sort_values(by = 'score', ascending= True)

plt.figure(figsize=(7,7))
plt.barh(top_group.index, top_group['score'])
plt.title('Group Features with the Highest Impact', fontsize = 15)
plt.savefig('selected features and impact socre.png');

"""C) Training Set Setup: Use 20%/80%"""

selected_cols = [feature for feature in top20_df['features']]
X = OMdb_ohed[selected_cols]
y = OMdb_ohed['target_revenue']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state = 2020, stratify = y)

def metrics_output(ml, name):
    pred = ml.predict(X_test)
    tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()
    train_score = ml.score(X_train, y_train)
    test_score = ml.score(X_test, y_test)
    precision = tp/(tp+fp)
    recall = tp/(fn+tp)
    f = 2*precision * recall/(precision + recall)
    print(f'{name} training accuracy socre: {train_score}')
    print(f'{name} testing accuracy socre: {test_score}')
    print(f'{name}precision socre: {precision}')
    print(f'{name} recall socre: {recall}')
    print(f'{name} f-measure: {f}')

"""a) Logistic Regression"""

Logreg = LogisticRegression()
Logreg.fit(X_train, y_train)

plot_confusion_matrix(Logreg, X_test, y_test, cmap = 'Blues')
plt.title('Logistic Regression Confusion Matrics')

metrics_output(Logreg, 'Logistic Regression')

plot_roc_curve(Logreg, X_test, y_test)
plt.plot(np.linspace(0, 1, 200),
         np.linspace(0, 1, 200),
         linestyle='--')
plt.title('Logistic Regression ROC/AUC')

"""b) SVM """

svm = SVC()

svm_params = {
    'C': np.linspace(0.0001, 1, 10),
    'kernel': ['rbf', 'linear']}

# use gridsearch to find out the best parameters
gs_svm = GridSearchCV(svm, svm_params, cv = 5)
gs_svm.fit(X_train, y_train)
gs_svm.best_params_

plot_confusion_matrix(gs_svm, X_test, y_test, cmap = 'Blues')
plt.title('SVM confusion matrix')

metrics_output(gs_svm, 'SVM with C = 1, kernel = rbf')

plot_roc_curve(gs_svm, X_test, y_test)
plt.plot(np.linspace(0, 1, 200),
         np.linspace(0, 1, 200),
         linestyle='--')
plt.title('SVM ROC/AUC');

"""c) KNN"""

knn = KNeighborsClassifier()

knn_params = {
    'n_neighbors': [3, 5, 8],
    'metric': ['minkowski', 'manhattan']
}

gs_knn = GridSearchCV(knn, knn_params, cv =5)
gs_knn.fit(X_train, y_train)
gs_knn.best_params_

plot_confusion_matrix(gs_knn, X_test, y_test, cmap = 'Blues')
plt.title('KNN confusion matrix')

metrics_output(gs_knn, 'KNN with k = 3, distance metric = minkowski')

plot_roc_curve(gs_knn, X_test, y_test)
plt.plot(np.linspace(0, 1, 200),
         np.linspace(0, 1, 200),
         linestyle='--')
plt.title('KNN ROC/AUC');

"""d) Naive Bayes"""

nb = MultinomialNB()
nb.fit(X_train, y_train)

plot_confusion_matrix(nb, X_test, y_test, cmap = 'Blues')
plt.title('Naive Bayes confusion matrix')

metrics_output(nb, 'Naive Bayes')

plot_roc_curve(nb, X_test, y_test)
plt.plot(np.linspace(0, 1, 200),
         np.linspace(0, 1, 200),
         linestyle='--')
plt.title('Naive Bayes ROC/AUC');

# Since KNN and Logistic Regression have the exact same accuracy score, 
# we go with Logistic Regression to do the cross validation because Logreg has a higher AUC score
cross_val_score(Logreg, X, y, cv =5)

cross_val_score(Logreg, X, y, cv =5).mean()

